{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9013e420-38d9-4ce7-a51e-d8852cebd3e5",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe6f5a8-5a48-4320-8ece-2f5459976955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "pd.options.display.max_columns=25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7b50c-de01-4d37-98a5-b5e0fbc338e4",
   "metadata": {},
   "source": [
    "## Reading and Merging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb48227b-cdc4-4e58-8b10-69dde82746a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2012_main = pd.read_csv('electricity_usage_data_2012.csv')\n",
    "data_2013_main = pd.read_csv('electricity_usage_data_2013.csv')\n",
    "data_2013_2_main = pd.read_csv('electricity_usage_data_2013_2.csv')\n",
    "data_2014_main = pd.read_csv('electricity_usage_data_2014.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de5b1ed-599c-48f5-bc1a-6a305ceec6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257848, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = [data_2012_main, data_2013_main, data_2013_2_main, data_2014_main]\n",
    "\n",
    "data = pd.concat(df_list)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7a4af-9be0-4e5c-9b47-06c454f403c8",
   "metadata": {},
   "source": [
    "Since we cleaned each df separetely there should not be any NANs, but checking for it nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e58bf68-5ec2-430c-8a1d-cf66679e8f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESID                200418\n",
       "Business Area       200418\n",
       "Service Address     200418\n",
       "Bill Type           200418\n",
       "Bill Date           200418\n",
       "Total Due ($)       200418\n",
       "kWh Usage           200418\n",
       "esid                 57430\n",
       "business_area        57430\n",
       "service_address      57430\n",
       "bill_type            57430\n",
       "bill_date            57430\n",
       "total_due            57430\n",
       "kwh_usage            57430\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc711ed-0575-4c95-abbf-2a6007afb0d9",
   "metadata": {},
   "source": [
    "### Checking for Duplicate Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9e183-34a6-4779-ab91-8a0e8979388a",
   "metadata": {},
   "source": [
    "Since there is an overlap in time period for the CSV files it is important not to have repeating rows.\n",
    "\n",
    "We can check for this in the following way: A particular ESID should be billed only once. Therefore by taking a subset of ESID, Business Area and Bill Date we can know if a particular customer's billing info has been repeated in the df or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ab3fae-9bd0-4028-b733-9cdb6c02ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_rows_index = data.duplicated(subset=['esid', 'business_area', 'service_address', 'bill_date'])\n",
    "(dup_rows_index).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535a098b-6701-46c0-a2b1-0dd2f607557b",
   "metadata": {},
   "source": [
    "This confirms the doubt that the overlap with the FY 2012 and FY 2013 with the CSV file has generated duplicate rows in the data.\n",
    "We need to remove these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "259664e9-bc0c-4cd3-a1e8-1809b596c35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144503, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_main = data[~(dup_rows_index)]\n",
    "\n",
    "data_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da959a24-aa55-4397-917a-162ed9ddfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main.to_csv('Electricity_Usage_Data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
