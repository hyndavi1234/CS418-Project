{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa37be33-33b4-4da7-8e6c-116ff5365062",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ae8758-c95e-44d9-8a5f-9c00dbaf68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "pd.options.display.max_columns=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531541fb-4015-4351-977d-3b6962c670c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main = pd.read_csv('Electricity_Usage_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1467a3-4893-4758-93cb-184469a595c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main[['bill_date']] = data_main[['bill_date']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979b7b8-81fd-4ea3-b523-5e1df9fc673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_main.loc[:,'bill_date'] = data_main['bill_date'].apply(lambda x: pd.to_datetime(f'{x.year}-{x.month}-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b619a71c-5944-4648-835a-a740d09a7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_enc = LabelEncoder()\n",
    "bill_type_enc = LabelEncoder()\n",
    "\n",
    "data_main['address_enc'] = address_enc.fit_transform(data_main['service_address'])\n",
    "data_main['bill_type_enc'] = bill_type_enc.fit_transform(data_main['bill_type'])\n",
    "data_main['year'] = data_main['bill_date'].apply(lambda x:x.year)\n",
    "data_main['month'] = data_main['bill_date'].apply(lambda x:x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e878518-9f8a-4d36-a300-ee0c3cbdf2c7",
   "metadata": {},
   "source": [
    "### Classification - Task: Predicting Type of Bill - Sharmisha\n",
    "\n",
    "Models proposed:\n",
    "1. Logistic Regression - widely used interpretable model which can be used for setting a baseline accuracy. This model assumed linear relationship between the variables, so mighht give bad results\n",
    "\n",
    "2. Decision Tree Classifier - It can handle the non-linear relationships well between input and target variable. Can be prone to overfitting on the train data.\n",
    "\n",
    "3. Random Forest Classifier - ensemble model, takes advantage of multiple decision trees to create a powerful model. But this model is not easy to interpret and requires more computational resource to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b65f1074-f364-4a38-9f64-19668cbfb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(model, X_train, X_test, y_train, y_test):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    print(f'Train F1 Score: {f1_score(y_train, y_train_pred, average=\"macro\")}')\n",
    "    print(f'Test F1 Score: {f1_score(y_test, y_test_pred, average=\"macro\")}')\n",
    "\n",
    "    print(f'Train Accuracy Score: {accuracy_score(y_train, y_train_pred)}')\n",
    "    print(f'Test Accuract Score: {accuracy_score(y_test, y_test_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db4e4214-c2ff-441f-97b3-7ba87680c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_main[[\n",
    "    'business_area', 'address_enc', 'kwh_usage', 'year', 'month'\n",
    "]]\n",
    "y = data_main[['bill_type_enc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "64287d45-9b7d-41fd-a695-c07fd1ed1156",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f52f7-8a2a-47d3-af88-98e738e9e2b4",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d4883217-3a3c-4be5-ac0f-d013d5ee9361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swarg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lreg = LogisticRegression().fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b2487862-48ad-45c9-87fb-ecc9df8d1f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.6435088261325143\n",
      "Test F1 Score: 0.6427111304198002\n",
      "Train Accuracy Score: 0.9986993634070143\n",
      "Test Accuract Score: 0.9986928446315129\n"
     ]
    }
   ],
   "source": [
    "classification_metrics(lreg, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014be23-0fee-46d8-8422-6939736bcdcb",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "344b3ec0-9f31-4220-85b7-25ac30a1043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4718a9cd-3771-48ff-9736-2fb1a219be86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.9998707454630322\n",
      "Test F1 Score: 0.6666403148167668\n",
      "Train Accuracy Score: 0.9999934641377237\n",
      "Test Accuract Score: 0.9998431413557816\n"
     ]
    }
   ],
   "source": [
    "classification_metrics(dtc, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ce3cb-115e-45c2-877e-3e19179e1976",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0299b583-f982-4b1d-a5d6-f6e346ef0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier().fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "77671173-429f-4e94-939b-b3edddb0bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score: 0.9998707454630322\n",
      "Test F1 Score: 0.6666403148167668\n",
      "Train Accuracy Score: 0.9999934641377237\n",
      "Test Accuract Score: 0.9998431413557816\n"
     ]
    }
   ],
   "source": [
    "classification_metrics(dtc, X_train, X_test, np.ravel(y_train), np.ravel(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1e544-8021-4451-b886-e4c9a113d832",
   "metadata": {},
   "source": [
    "#### Preliminary Conclusions\n",
    "\n",
    "In the above models based although the accuracy seems quite high for all the models, the important metric here is the F1 score. Since it gives a better understanding of the results. Logistic Classification model is struggling to capture the features well since the F1 is low for the train set too. \n",
    "\n",
    "The decision tree classifier however is able to perform well on the train dataset while taking a huge hit on the test set. This implies that is likely overfitting. The case is similar in the random forest classifier too. One possible reason for this could be the class imbalance that exists in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
